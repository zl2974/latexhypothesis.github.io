---
title: "R Code for hypothesis testing"
author: "Jeffrey Liang"
date: "10/29/2020"
output: 
  html_document:
    toc: true
    toc_float : true
    code_folding: show
---

```{css, echo=FALSE}
pre {
  max-height: 350px;
  overflow-y: auto;
}

pre[class] {
  max-height: 300px;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval = T)
library(tidyverse)
```


# Distribution {.tabset}

## Binomial
$P(\mathcal{X}=k)\ = {n \choose k}p^k(1-p)^{n-k}$
```{r collapse=T}
rbinom(5,10,0.3)
qbinom(0.05,10,0.3)
pbinom(1,10,0.3)
dbinom(1,10,0.3)
```


## Normal Distribution

$P(\mathcal{X}=k)\ = \frac{1}{\sqrt{2\pi}\sigma}*e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
```{r collapse=T}
rnorm(5)
qnorm(0.05)
pnorm(1.96)
dnorm(1.96)
```

# Descriptive Statistics {.tabset}

## hide
## Table
```{r}


################################################################
#                  Biostatistical Methods I                    #
#                   Descriptive Statistics                     #
#                    Author: Cody Chiuzan                      #
################################################################

# Library 'arsenal' is used for descriptive statistics tables
# Library 'dplyr' has nice functions for data manipulation, also mutate()
# Library 'ggplot2' is used for graphing


library(arsenal)
library(dplyr)
library(ggplot2)

#########################################################################
#                          Import Data                                  #
#########################################################################

# Set working directory

low_birth_all <- read.csv(here::here("R_Code/R - Module 2/lowbwt_ALL.csv"))

names(low_birth_all)
head(low_birth_all)

dim(low_birth_all)
summary(low_birth_all)
str(low_birth_all)

# Check for missing values
anyNA(low_birth_all)

filter(low_birth_all, is.na(age))

# Some details about the data: 189 births info were collected at a medical center.
# The dataset contains the following 10 variables:
# low: indicator of birth weight less than 2.5kg
# age: mother's age in years
# lwt: mother's weight in pounds at last menstrual period
# race: mothers race ("white", "black", "other")
# smoke: smoking status during pregnancy (yes/no)
# ht: history of hypertension (yes/no)
# ui: presence of uterine irritability (yes/no)
# ftv: physician visit during the first trimester (yes/no)
# ptl: previous premature labor (yes/no)
# bwt: birth weight in grams


#########################################################################
#         Descriptive Statistics: Continuous Variables                  #
#########################################################################


mean(low_birth_all$age)                               # Mean
median(low_birth_all$age)                             # Median
sd(low_birth_all$age)                                 # Standard Deviation
quantile(low_birth_all$age)                           # Min, 25ht, 50th, 75th, Max
quantile(low_birth_all$age, c(0.10, 0.30, 0.60))        # Tertiles

# A more condensed way to obtain summary statistics
summary(low_birth_all$age)

# Summary statistics for each level of another categorical variable
mean <- tapply(low_birth_all$bwt, low_birth_all$race, mean)
sd <- tapply(low_birth_all$bwt, low_birth_all$race, sd)
med <- tapply(low_birth_all$bwt, low_birth_all$race, median)
min <- tapply(low_birth_all$bwt, low_birth_all$race, min)
max <- tapply(low_birth_all$bwt, low_birth_all$race, max)
cbind(mean, sd, med, min, max)

# Use function tableby() from library 'arsenal' to create a summary table (called Table 1 in publications)
# Use continuous and categorical variables


# First table - not ideal
tab1 <- tableby(~ age + bwt + smoke, data = low_birth_all)
summary(tab1)


# Change variable names/labels
my_labels <-
  list(
    age = "Age(yrs)",
    bwt = "Birthweight(g)",
    smoke = "Smoker",
    race = "Race"
  )

# Clean the output
my_controls <- tableby.control(
  total = T,
  test = F,
  # No test p-values yet
  numeric.stats = c("meansd", "medianq1q3", "range", "Nmiss2"),
  cat.stats = c("countpct", "Nmiss2"),
  stats.labels = list(
    meansd = "Mean (SD)",
    medianq1q3 = "Median (Q1, Q3)",
    range = "Min - Max",
    Nmiss2 = "Missing",
    countpct = "N (%)"
  )
)

# Make 'smoke' a factor to show N (%)
birth_df <- low_birth_all %>%
  mutate(smoke = factor(smoke, labels = c("No", "Yes"))) # Start labeling with 0 (increasing order)

# Second table
tab2 <-
  tableby(~ age + bwt + smoke, data = birth_df, control = my_controls)
summary(tab2,
        title = "Descriptive Statistics: Lowbirth Data",
        labelTranslations = my_labels,
        text = T)

# Tabulation by race categories
tab3 <-
  tableby(race ~ age + bwt + smoke, data = birth_df, control = my_controls)
summary(tab3,
        title = "Descriptive Statistics: Lowbirth Data",
        labelTranslations = my_labels,
        text = T)



#########################################################################
#         Descriptive Statistics: Categorical Variables                 #
#########################################################################

tbl <-
  table(low_birth_all$smoke, low_birth_all$race)	        # Two-way table
tbl
prop.table(tbl, 1)                        		                # Row proportions
prop.table(tbl, 2)                                            # Column proportions

# 3-way cross-tabulation
xtabs( ~ race + smoke + ht, data = low_birth_all)

```

# T-Test {.tabset}

## hide

## One Group 

```{r eval = T}
################################################################
#                    Biostatistical Methods I                  #
#            Statistical Inference: One-Sample Mean            #
#                     Author: Cody Chiuzan                     #
################################################################

############################################################
#               Sample mean distributions: CLT             #
############################################################

# Draw 1000 samples of size 10 from an underlying exponential distribution with parameter lambda=0.3
# Calculate their means/var and draw a histogram to vizualize the sample means distribution

set.seed(2)
sample_means_exp1 = rep(NA, 1000)

for (i in 1:1000) {
  sample_means_exp1[i] = mean(rexp(10, 0.3))
}

# sample_means_exp

# Calculate the means and the variances of all samples
mean(sample_means_exp1)                # compare to true Mean = 1/lambda
var(sample_means_exp1)                 # compare to true Var=1/lambda^2

#Histogram
hist(sample_means_exp1,
     main = "Samples of Size N=10 from Exp(0.3)",
     xlab = "Sample Means",
     prob = T)
lines(density(sample_means_exp1), col = "darkblue", lwd = 2)

# Draw 1000 samples of size 50 from an underlying exponential distribution with parameter lambda=0.3
# Calculate their means/var and draw a histogram to vizualize the sample means distribution

set.seed(2)
sample_means_exp2 = rep(NA, 1000)

for (i in 1:1000) {
  sample_means_exp2[i] = mean(rexp(50, 0.3))
}

# Calculate the means and the variances of all samples
mean(sample_means_exp2)                # compare to true Mean = 1/lambda
var(sample_means_exp2)                 # compare to true Var=1/lambda^2

#Histogram
hist(sample_means_exp2,
     main = "Samples of Size N=50 from Exp(0.3)",
     xlab = "Sample Means",
     prob = T)
lines(density(sample_means_exp2), col = "darkblue", lwd = 2)



# Construct a 95% CI for the population mean with n=10, X_bar=175, and known sigma=15
# Sigma represents the pooulation standard deviation
# 1-(alpha/2)=1-(0.05/2)=0.975

LCLz95 <- 175 - qnorm(0.975) * 15 / sqrt(10)
UCLz95 <- 175 + qnorm(0.975) * 15 / sqrt(10)
CLz95 <- c(LCLz95, UCLz95)
CLz95

# What if we want a 99% CI?
LCLz99 <- 175 - qnorm(0.995) * 15 / sqrt(10)
UCLz99 <- 175 + qnorm(0.995) * 15 / sqrt(10)
CLz99 <- c(LCLz99, UCLz99)
CLz99


# Construct a 95% CI for the population mean with n=10 => df=10-1=9, X_bar=175, and known s=15
# s represents the sample standard deviation

LCLt95 <- 175 - qt(0.975, df = 9) * 15 / sqrt(10)
UCLt95 <- 175 + qt(0.975, df = 9) * 15 / sqrt(10)
CLt95 <- c(LCLt95, UCLt95)
CLt95


# Construct a 95% CI for the population variance with known s=15
# s represents the sample standard deviation

LCL_var95 <- 9 * (15 ^ 2) / qchisq(0.975, 9)
UCL_var95 <- 9 * (15 ^ 2) / qchisq(0.025, 9)
CL_var95 <- c(LCL_var95, UCL_var95)
CL_var95


# Hypothesis Test: Infarct size example
# Test if the mean infract size is different from 25
# X_bar=16, s=10, N=40

t_stats <- (16 - 25) / (10 / sqrt(40))
t_stats

# Compare the test statistics with the critical value, alpha=0.05

qt(0.975, 39) # 2.02

# Compute the p-value: t_stats<0, so the p-value is twice area to the left of a t distr. with 39 df

p.val <- 2 * pt(t_stats, 39) # p.val<.0001, reject H0.


# Remember the low_birth data

low_birth_all <-
  read.csv(here::here("R_Code/R - Module 2/lowbwt_ALL.csv"))

# Let's test if the true mean is different than 3000g
# One-sample t-test, two-tailed

t.test(low_birth_all$bwt, alternative = 'two.sided', mu = 3000)


# Output from R

# One Sample t-test

# t = -1.0437, df = 188, p-value = 0.298 ----> Fail to reject H0.
# alternative hypothesis: true mean is not equal to 3000
# 95 percent confidence interval: 2840.049 3049.264
# sample estimates: mean of x is 2944.656


# Let's test if the true mean is less than 3000g
# One-sample t-test, one-tailed

t.test(low_birth_all$bwt, alternative = 'less', mu = 3000)

# Output from R
# t = -1.0437, df = 188, p-value = 0.149 ----> Fail to reject H0.
# alternative hypothesis: true mean is less than 3000
# 95 percent confidence interval: -Inf 3032.312  (One-sided confidence interval)
# sample estimates: mean of x is 2944.656 
```


## Two Group
```{r}


################################################################
#                     Biostatistical Methods I                 #
#            Statistical Inference: Two-Sample Means           #
#           Author: Cody Chiuzan; Date: Sept 23, 2019          #
################################################################



###########################################################################
#   Conduct a two-sample paired t-test to assess the effect of a new diet #
###########################################################################

weight_before <- c(201, 231, 221, 260, 228, 237, 326, 235, 240, 267, 284, 201)
weight_after <- c(200, 236, 216, 233, 224, 216, 296, 195, 207, 247, 210, 209)
weight_diff <- weight_after - weight_before
sd_diff <- sd(weight_diff)


test_weight <- mean(weight_diff) / (sd_diff / sqrt(length(weight_diff)))

# Use the t.test() built-in function
# What alternative are you testing?

t.test(weight_after,
       weight_before,
       paired = T,
       alternative = "less")

# R output

# data:  weight_after and weight_before
# t = -3.0201, df = 11, p-value = 0.005827
# alternative hypothesis: true difference in means is less than 0
# 95 percent confidence interval:  -Inf -8.174729
# sample estimates: mean of the differences -20.16667

# Reject the null and conclude that the mean LDL levels are significantly lower after the diet.


###########################################################################################################
#   Conduct a two-sample independent t-test to assess the differences in BMD b/w the OC and non-OC groups #
###########################################################################################################

# Oral contraceptive example
# Testing equality of variances for two independent samples
# drawn from two underlying normal distributions.

# Sample 1: s1=0.16, n1=10, x1_bar=1.08
# Sample 2: s2=0.14, n2=10, x2_bar=1.00

F_test <- 0.16 ^ 2 / 0.14 ^ 2

F_crit <- qf(.975, df1 = 9, df2 = 9)

# Compare the F statistic (F_test) to the critical value
# Fcrit: F with 9 dfs in numerator and 9 dfs in denominator
# Because F_test < F_crit, we fail to reject and conclude
# that the pop. variances are not significantly different.

# Use two-sample t-test with equal variances.
std_pooled <- sqrt(((0.16 ^ 2 * 9) + (0.14 ^ 2 * 9)) / 18)

t_stats <- (1.08 - 1.00) / (std_pooled * sqrt((1 / 10) + (1 / 10)))

# Compare t_stats to the critical value: t with 18 df

qt(0.975, 18) # 2.10

# t-stats=1.19 < 2.10, fail to reject the null and conclude that
# there is not a sig difference between the mean BMD levels of the two groups.

# 95% CI is your practice!


################################################################
#               Two-Sample independent t-test                  #
################################################################

# Effect of caffeine on muscle metabolism.
# 15 men were randomly selected to take a capsule containing pure caffeine one hour before the test.
# The other group of 20 men received a placebo capsule.
# During each exercise the subject's respiratory exchange ratio (RER) was measured.
# The question of interest to the experimenter was whether, on average, caffeine consumption has an effect on RER.
# The two samples came from two underlying normal distributions: N(94.2,5.6), N(105.5,8.1) and are independent.


# Ideally, you should generate data using past info (here I made it up).

set.seed(6)
caff <- rnorm(15, 94.2, 5.6)
placebo <- rnorm(20, 105.5, 8.1)

# Test equality of variances: use R function var.test()

var.test(placebo, caff, alternative = "two.sided")

#F = 3.5768, num df = 19, denom df = 14, p-value = 0.01881             # Reject the null, evidence that variances are not equal.


res <-
  t.test(caff, placebo, var.equal = FALSE, paired = FALSE)               # var.equal=FALSE is the default, so no need to specifically write it.
res

# Look at the complete list of results.
names(res)

# t = -4.5834, df = 30.125, p-value = 7.472e-05                        # Reject the null and conclude that the means RER are sig diff b/w the two groups.

# 95% CI of the difference: (-17.639668,-6.766628)                     # We could safely say that the mean RER is sig. lower in the caffeine group (Why?)

```


# Multigroup Camparison {.tabset}

## hide

## ANOVA

```{r eval = T}
################################################################
#                      Biostatistical Methods I                #
#             One-Way Analysis of Variance (ANOVA)             #
#                      Author: Cody Chiuzan                    #
################################################################

################################################################
# A study is examining the effect of glucose on insulin release.
# Specimens of pancreatic tissue from experimental animals were
# treated with five different stimulants and the insulin levels were recorded.
# Use an ANOVA test to compare the mean insulin levels across the five groups.

################################################################

ins1 <- c(1.53, 1.61, 3.75, 2.89, 3.26)
ins2 <- c(3.15, 3.96, 3.59, 1.89, 1.45, 1.56)
ins3 <- c(3.89, 3.68, 5.70, 5.62, 5.79, 5.33)
ins4 <- c(8.18, 5.64, 7.36, 5.33, 8.82, 5.26, 7.10)
ins5 <- c(5.86, 5.46, 5.69, 6.49, 7.81, 9.03, 7.49, 8.98)

# Re-shape the data
insulin <- c(ins1, ins2, ins3, ins4, ins5)
ind <-
  c(rep(1, length(ins1)),
    rep(2, length(ins2)),
    rep(3, length(ins3)),
    rep(4, length(ins4)),
    rep(5, length(ins5)))

new_data <- as.data.frame(cbind(insulin, ind))
head(new_data)

# Summarize the data

tmp_functn <-
  function(x)
    c(
      sum = sum(x),
      mean = mean(x),
      var = var(x),
      n = length(x)
    )
tapply(insulin, ind, tmp_functn)

# Create box-plots
boxplot(
  insulin ~ ind,
  data = new_data,
  main = "Effect of glucose on insulin release",
  xlab = "Experimental Group",
  ylab = "Insulin levels"
)

# Perform an ANOVA test: are the mean insulin levels significantly different?
# Need to mention the independent variable as a factor; o/w will be considered continuous
# Function lm() is broader, including linear regression models
res <- lm(insulin ~ factor(ind), data = new_data)

# Coefficients of the ANOVA model with 'grand mean' and alpha effects.
# Will use them later in regression.
res

# Our regular ANOVA table with SS, Mean SS and F-test
anova(res)


# Another option using aov();
# Save the anova object to use later for multiple comparisons
res1 <- aov(insulin ~ factor(ind), data = new_data)
summary(res1)
```

## Between Groups Comparison
```{r message=F}
library(multcomp)
```

```{r}
# Multiple comparisons adjustments: includes Bonferroni, Holm, Benjamini-Hochberg
pairwise.t.test(new_data$insulin, new_data$ind, p.adj = 'bonferroni')

# For Tukey, we need to use another function with an object created by aov()
Tukey_comp <- TukeyHSD(res1)
Tukey_comp
plot(Tukey_comp)


# Dunnett's test: multiple comparisons with a specified control (here group #1)
summary(glht(res1), linfct = mcp(Group = "Dunnett"))
```


# Proportion {.tabset}

## hide

## Normal Approximate Binomial
```{r eval = T}
################################################################
#                   Biostatistical Methods I                   #
#               Inferences for One-Sample Proportions          #
#                      Author: Cody Chiuzan                    #
################################################################

# Normal Approximation: Observe the shape of different Binomial distributions with varying n and p.

#1
par(mfrow = c(1, 2))
plot(
  0:50,
  dbinom(0:50, 10, 0.5),
  type = 'h',
  ylim = c(0, 0.50),
  xlab = 'X',
  main = 'Bin(10,0.5)',
  ylab = 'P(X)',
  lwd = 3,
  cex.lab = 1.5,
  cex.axis = 2,
  cex.main = 2
)

plot(
  0:50,
  dbinom(0:50, 30, 0.5),
  type = 'h',
  ylim = c(0, 0.50),
  xlab = 'X',
  main = 'Bin(30,0.5)',
  ylab = 'P(X)',
  lwd = 3,
  cex.lab = 1.5,
  cex.axis = 2,
  cex.main = 2
)

#2
par(mfrow = c(1, 2))
plot(
  0:50,
  dbinom(0:50, 10, 0.10),
  type = 'h',
  ylim = c(0, 0.50),
  xlab = 'X',
  main = 'Bin(10,0.1)',
  ylab = 'P(X)',
  lwd = 3,
  cex.lab = 1.5,
  cex.axis = 2,
  cex.main = 2
)

plot(
  0:50,
  dbinom(0:50, 50, 0.10),
  type = 'h',
  ylim = c(0, 0.50),
  xlab = 'X',
  main = 'Bin(50,0.1)',
  ylab = 'P(X)',
  lwd = 3,
  cex.lab = 1.5,
  cex.axis = 2,
  cex.main = 2
)

#3
par(mfrow = c(1, 2))
plot(
  0:50,
  dbinom(0:50, 10, 0.02),
  type = 'h',
  ylim = c(0, 0.8),
  xlab = 'X',
  main = 'Bin(10,0.02)',
  ylab = 'P(X)',
  lwd = 3,
  cex.lab = 1.5,
  cex.axis = 2,
  cex.main = 2
)

plot(
  0:50,
  dbinom(0:50, 50, 0.02),
  type = 'h',
  ylim = c(0, 0.8),
  xlab = 'X',
  main = 'Bin(50,0.02)',
  ylab = 'P(X)',
  lwd = 3,
  cex.lab = 1.5,
  cex.axis = 2,
  cex.main = 2
)

#4
par(mfrow = c(1, 2))
plot(
  0:50,
  dbinom(0:50, 10, 0.95),
  type = 'h',
  ylim = c(0, 0.8),
  xlab = 'X',
  main = 'Bin(10,0.95)',
  ylab = 'P(X)',
  lwd = 3,
  cex.lab = 1.5,
  cex.axis = 2,
  cex.main = 2
)

plot(
  0:50,
  dbinom(0:50, 50, 0.95),
  type = 'h',
  ylim = c(0, 0.8),
  xlab = 'X',
  main = 'Bin(50,0.95)',
  ylab = 'P(X)',
  lwd = 3,
  cex.lab = 1.5,
  cex.axis = 2,
  cex.main = 2
)

################################################################
# In a survey of 300 randomly selected drivers, 125 claimed that
# they regularly wear seat belts. Can we conclude from these data
# that the population proportion who regularly wear seat belts is 0.50?
# Perform a hypothesis test and
# Construct a 95% confidence interval for the true population proportion.
################################################################

# p_hat=125/300
# p0=0.50


# Prop.test performs a chi-squared test and not a z-test.
prop.test(125, 300, p = 0.5)


# Create your own function to perform a one-sample proportion test
# and create a 100(1-alpha) CI using the Normal Approximation

one.proptest_norm <-
  function(x,
           n,
           p = NULL,
           conf.level = 0.95,
           alternative = "less") {
    # x the number of 'cases' in the sample
    # n the total sample size
    # p is the hypothesized value
    
    z.stat <- NULL
    cint <- NULL
    p.val <- NULL
    phat <- x / n
    qhat <- 1 - phat
    
    if (length(p) > 0) {
      q <- 1 - p
      SE.phat <- sqrt((p * q) / n)
      z.stat <- (phat - p) / SE.phat
      
      p.val <- pnorm(z.stat)
      
      if (alternative == "two.sided") {
        p.val <- p.val * 2
      }
      
      if (alternative == "greater") {
        p.val <- 1 - p.val
      }
    } else {
      # Construct a confidence interval
      SE.phat <- sqrt((phat * qhat) / n)
    }
    cint <-
      phat + c(-1 * ((qnorm(((1 - conf.level) / 2
      ) + conf.level)) * SE.phat),
      ((qnorm(((1 - conf.level) / 2
      ) + conf.level)) * SE.phat))
    
    return(list(
      estimate = phat,
      z.stat = z.stat,
      p.val = p.val,
      cint = cint
    ))
  }

# In our example:

one.proptest_norm(125, 300, 0.5, alternative = "two.sided")

# P-hat estimate
#0.417

# Z-statistic: z.stat
# -2.886

#$p.val
#0.004

# 95 % CI: (0.360, 0.473)


####################################################################
#       Perform an Exact test, no normal approximation             #
#          This function uses Clopper-Pearson method               #
####################################################################

binom.test(125,
           300,
           p = 0.5,
           alternative = "two.sided",
           conf.level = 0.95)

# 95% Exact CI: (0.360, 0.474)

# Exact p-value: 0.004
```


## Contingency Table Method
```{r eval = T}
#################################################################
#                      Biostatistical Methods I                 #
#          Contingency Tables: Tests for Categorical Data       #
#                          Author: Cody Chiuzan                 #
#################################################################

################################################################
#                      Chi-Squared Test                        #
################################################################

# Marijuana usage among colleg students
# Chi-squared test for homogeneity

drug_data <-
  matrix(
    c(57, 50, 43, 57, 58, 20, 56, 45, 24, 45, 22, 33),
    nrow = 4,
    ncol = 3,
    byrow = T,
    dimnames = list(
      c("freshman", "sophomore", "junior", "senior"),
      c("experimental", "casual", "modheavy")
    )
  )

drug_data

chisq.test(drug_data)

# X-squared = 19.369, df = 6, p-value = 0.003584
# Critical value: qchisq(0.95,6) = 12.59

# We reject the null hypothesis and conclude that the proportions of marijuana usage are different among classes


###
# Association b/w pelvic inflammatory disease and ectopic pregnancy
# Chi-squared test for independence

preg_data <- matrix(
  c(28, 6, 251, 273),
  nrow = 2,
  ncol = 2,
  byrow = T,
  dimnames = list(c("PID", "No PID"),
                  c("Ect Preg", "No Ect Preg"))
)


preg_data

chisq.test(preg_data)
# Get the expected values: chisq.test(preg_data)$expected


# X-squared with Yates' correction
# X-squared=13.81, df=1, p-value=0.0002
# Critical value: qchisq(0.95,1) = 3.84

# We reject the null and conclude that there is sufficient evidence that PID and ectopic pregnancy are associated.


###
# What if we have raw data? How do we perform a chi-squared test?
# Use R data 'quine' (library MASS) to compare gender distribution between ethnicities.

library(MASS)
data(quine)
names(quine)

# Create a 2x2 table
table(quine$Sex, quine$Eth)

# Compute row percentages
prop.table(table(quine$Sex, quine$Eth), 1)

# Chi-squared without continuity correction.
chisq.test(table(quine$Sex, quine$Eth), correct = F)

# X-squared = 0.004, df = 1, p-value = 0.949. Not a significant difference.

# Chi-squared with continuity correction.
chisq.test(table(quine$Sex, quine$Eth), correct = T)

# X-squared ~ 0, df = 1, p-value = 1. Not a significant difference.

#################################################################
#                      Biostatistical Methods I                 #
#          Contingency Tables: Tests for Categorical Data       #
#                          Author: Cody Chiuzan                 #
#################################################################

###############################################################
#      Fisher's Exact test for small cell counts (Eij < 5)    #
###############################################################

# Tea-time experiment
# One-tailed test: calculations were also made in the lecture notes

tea_exp <- matrix(c(3, 1, 1, 3), nrow = 2)

fisher.test(tea_exp, alternative = "greater")
# P-val=0.2429, fail to conclude discriminating ability

# Two-sided Fisher test
fisher.test(tea_exp)
# P-val=0.4857
# CI is for the odds ratio, not difference in proportions

# What if we used a chi-square instead? Not ideal as the exp freq are <5
chisq.test(tea_exp)
# P-val=0.4795

# Notice that the two-sided p-value from Fisher is greater than the one generated by chi-square
# This supports the conclusion that Fisher Exat Test is more conservative (harder to reject)



practice_data <- matrix(c(1, 8, 9, 3), nrow = 2,
                        dimnames = list(c("diet", "non-diet"), c("men", "women")))

chisq.test(practice_data)
# Pearson's Chi-squared test with Yates' continuity correction for 2X2 table
# X-squared = 6.0494, df = 1, p-value = 0.01391
# Warning message:
# In chisq.test(practice_data) : Chi-squared approximation may be incorrect

fisher.test(practice_data)
# Fisher's Exact Test for Count Data
# p-value = 0.007519  # Notice the difference in p-values b/w chi-squared and Fisher



###############################################################
#      McNemar Test for binomial matched-pair data            #
#               Normal approximation                          #
###############################################################

# Two procedures are tested on the same 75 subjects
# in order to identify the absence/presence of the disease

procedure_data <- matrix(
  c(41, 8, 14, 12),
  nrow = 2,
  byrow = T,
  dimnames = list(c("positive", "negative"), c("positive", "negative"))
)

mcnemar.test(procedure_data)

# McNemar's Chi-squared test with continuity correction
# McNemar's Chi-squared = 1.1364, df = 1, p-value = 0.2864

# What if you performed a chi-squared test instead?
chisq.test(procedure_data)

# X-squared = 6.278, df = 1, p-value = 0.01222
# Notice that the conclusions would be totally different.
```

# Regression {.tabset}

## Hide

## SLR
```{r regresssion}
################################################################
#                     Biostatistical Methods I                 #
#                     Simple Linear Regression                 #
################################################################


library(faraway)
library(broom)
library(dplyr)

# Load data diabetes
data(diabetes)
names(diabetes)
summary(diabetes)

# Plot (Y) chol vs age (X)
plot(diabetes$age, diabetes$chol)

reg_diab<-lm(diabetes$chol~diabetes$age)

# Summarize regression
summary(reg_diab)

tidy(reg_diab)

glance(reg_diab)

# Regression objects
names(reg_diab)

# Get fitted.values
reg_diab$fitted.values

# Scatterplot and regression line overlaid
plot(diabetes$age, diabetes$chol)
abline(reg_diab,lwd=2,col=2)


# Calculate the regression coefficient estimates 'by hand'.
set.seed(1)

data = data.frame(x = rnorm(30, 3, 3)) %>% mutate(y = 2+.6*x +rnorm(30, 0, 1))
linmod = lm(y~x, data = data)

summary(linmod)


beta1 = with(data, sum((x - mean(x))*(y - mean(y))) / sum((x - mean(x))^2))
beta0 = with(data, mean(y) - beta1*mean(x))
c(beta0, beta1)

# Notice the same values

################################################################
#                     Biostatistical Methods I                 #
#             Simple Linear Regression - Inferences            #
################################################################
# Load libraries
library(faraway)
library(broom)
library(dplyr)

# Read data 'Hospitals'
data_hosp<-read.csv(here::here("R_Code","R - Module 17","Hospital.csv"))
names(data_hosp)

# Look at data structure
str(data_hosp)

# Scatter plot (Y) vs (X)
# LOS: length of stay(Y)
# BEDS: number of beds(X)

data_hosp %>% 
   ggplot(aes(BEDS, LOS)) + geom_point(color='blue') + theme_bw(base_size=20) + 
   labs(x="Number of beds", y="Length of stay (days)")

# Simple linear regression
reg_hos<-lm(data_hosp$LOS~data_hosp$BEDS)

# Analyze the regression results
summary(reg_hos)

# Get the ANOVA table
anova(reg_hos)

# Residual st error: MSE=sigma^2
glance(reg_hos)$sigma


# Scatter plot with regression line overlaid  
data_hosp %>% 
  ggplot(aes(BEDS, LOS)) + geom_point(color='blue') + theme_bw(base_size=20) +
  geom_smooth(method='lm', se=FALSE, color='red') +
  labs(x="Number of beds", y="Length of stay (days)")

# Scatter plot with regression line overlaid and 95% confidence bands
data_hosp %>% 
  ggplot(aes(BEDS, LOS)) + geom_point(color='blue') + theme_bw(base_size=20) +
  geom_smooth(method='lm', se=TRUE, color='red') +
  labs(x="Number of beds", y="Length of stay (days)")


# How do we calculate the 95% CI for the slope?
# Interpretation: 95% CI for the expected/mean difference in LOS for 1 bed differene

# Get the critical t value for alpha=0.05 and n-2 df

qt(0.975,111)  # In data hospital, df=n-2=113-2=111

coef<-summary(reg_hos)$coefficients[2,1] 
err<-summary(reg_hos)$coefficients[2,2] 
slope_int<-coef + c(-1,1)*err*qt(0.975, 111)

# CIs for both slope and intercept
confint(reg_hos)
confint(reg_hos,level=0.95)


# How do we calculate the 95% CI for 100 beds difference?
coef<-summary(reg_hos)$coefficients[2,1] 
err<-summary(reg_hos)$coefficients[2,2] 
slope_int100<-100*coef + c(-1,1)*(100*err)*qt(0.975, 111)
slope_int100


#############################################################################
# Calculate 95% CIs using predict function
# If 'newdata' is omitted the predictions are based on the data used for the fit, like in the case below.

pred.clim <- predict.lm(reg_hos, interval="confidence") 
datapred <- data.frame(cbind(data_hosp$BEDS, data_hosp$LOS, pred.clim))

plot(datapred[,1],datapred[,2],xlab="Number of Beds", ylab="Length of stay (days)")

#abline(reg_hos,lwd=2,col=2)
lines(datapred[,1],datapred[,3], lwd=2)
lines(datapred[,1],datapred[,5], lty=1, col=3, type='l')
lines(datapred[,1],datapred[,4], lty=1, col=3,type='l')

# Calculate 95% PIs for fitted values using predict function
# Compare to prediction intervals: of course that the PIs are wider than CIs
pred.plim <- predict.lm(reg_hos, interval="prediction") 
datapred1 <- data.frame(cbind(data_hosp$BEDS, data_hosp$LOS, pred.plim))

#abline(reg_hos,lwd=2,col=2)
lines(datapred1[,1],datapred1[,3], lwd=2)
lines(datapred1[,1],datapred1[,5], lty=1, col=2, type='l')
lines(datapred1[,1],datapred1[,4], lty=1, col=2,type='l')


##############################################################
# Calculate the correlation coefficient between LOS and BEDS
cor(data_hosp$LOS, data_hosp$BEDS)

# Look at the R_squared. How does it compare to the correlation? Same value, but only for SLR.
cor(data_hosp$LOS, data_hosp$BEDS)^2

```

## MLR
```{r mlr}
################################################################
#                   Biostatistical Methods I                   #
#                 Multiple Linear Regression                   #
################################################################

library(faraway)
library(broom)
library(dplyr)

# Read data 'Hospitals'
data_hosp<-read.csv(here::here("R_Code/R - Module 17/Hospital.csv"))
names(data_hosp)

# Scatter plot with regression line overlaid and 95% confidence bands
data_hosp %>% 
  ggplot(aes(BEDS, LOS)) + geom_point(color='blue') + theme_bw(base_size=20) +
  geom_smooth(method='lm', se=TRUE, color='red') +
  labs(x="Number of beds", y="Length of stay (days)")


# Simple linear regression: Length of stay (LOS) vs number of BEDS 
reg_hos<-lm(data_hosp$LOS~data_hosp$BEDS)
summary(reg_hos)

# Get the ANOVA table
anova(reg_hos)


# Matrix model
model.matrix(reg_hos)

model.matrix(reg_hos) %>% head


# Multiple linear regression: 
# Var 1: Number of BEDS 
# Var 2: INFRISK (prob. % of getting an infection during hospitalization)

regmult1_hos<-lm(data_hosp$LOS~data_hosp$BEDS + data_hosp$INFRISK)

# Analyze the regression results
summary(regmult1_hos)


# Multiple linear regression: BEDS and INFRISK and NURSE
regmult2_hos<-lm(data_hosp$LOS~data_hosp$BEDS + data_hosp$INFRISK+data_hosp$NURSE)
summary(regmult2_hos)



# Multiple linear regression: BEDS and MEDSCHL (Medical School Affiliation: 1-Yes, 2-No)

# Recode MEDSCHL: Yes:1 and No:0

data_hosp$MS<-ifelse(data_hosp$MEDSCHL==1,1,ifelse(data_hosp$MEDSCHL==2, 0, NA))

# Multiple linear regression: INFRISK and new MS (Medical School Affiliation: 1-Yes, 0-No)
regmult3_hos<-lm(data_hosp$LOS~data_hosp$INFRISK +data_hosp$MS)
summary(regmult3_hos)


# Categorical predictor REGION: multiple levels

# Simple linear regression with predictor REGION (1:NE, 2:NC, 3:S, 4:W)
data_hosp %>% lm(LOS~REGION, data=.) %>% summary
# How does it look?

# Make it a factor
data_hosp %>% lm(LOS~factor(REGION), data=.) %>% summary

# Compare intercept model
data_hosp %>% lm(LOS~factor(REGION), data=.) %>% summary

# To No intercept model
data_hosp %>% lm(LOS~0+factor(REGION), data=.) %>% summary


# Change the reference category for REGION (from 1 to 3)
# Intercept added
data_hosp %>% mutate(REGION=relevel(factor(REGION),ref=3)) %>% lm(LOS~factor(REGION), data=.) %>% summary



# Multiple linear regression: INFRISK, new MS and Region (1:NE, 2:NC, 3:S, 4:W)
regmult4_hos<-lm(data_hosp$LOS~data_hosp$INFRISK +data_hosp$MS+factor(data_hosp$REGION))
summary(regmult4_hos)

# 'General' global test for all predictors
anova(regmult4_hos)


# Multiple linear regression: new MS, Region (1:NE, 2:NC, 3:S, 4:W) and their interaction
regmult5_hos<-lm(LOS~INFRISK*MS, data=data_hosp)
summary(regmult5_hos)

# Vizualize interaction for reg5: LOS vs INFRISK by MS affiliation
qplot(x = INFRISK, y = LOS, data = data_hosp, color = factor(MS)) +
     geom_smooth(method = "lm", se=FALSE) +
     labs(x="Infection Risk", y="Length of stay (days)")
                 
# Lines look fairly parallel, in line with the non-sig interaction result.

```

## Diagonsis
```{r diagonsis}
################################################################
#                   Biostatistical Methods I                   #
#                  Multiple Linear Regression                  #
#                     Model Diagnostics                        #
################################################################

library(dplyr)
library(HH)


# Read data Surgical.csv
data_surg<-read.csv(here::here("R_Code/R - Module 20/Surgical.csv"))
names(data_surg)
#attach(data_surg)

# Residuals vs fitted values plot
par(mfrow = c(1, 2))

fit1 <- lm(Survival ~ Bloodclot + Progindex + Enzyme + Liver, data=data_surg)
plot(fitted(fit1), resid(fit1), xlab = "Predicted/Fitted value", ylab = "Residual")
title("(a) Residual Plot for Y (Survival) ")
abline(0, 0)

# MLR with LnSurvival - natural log transformation of "Survival" outcome
fit2 <- lm(Lnsurvival ~ Bloodclot + Progindex + Enzyme + Liver,  data=data_surg)
plot(fitted(fit2), resid(fit2), xlab = "Predicted/Fitted value", ylab = "Residual")
title("(b) Residual Plot for lnY (LnSurvival)")
abline(0, 0)


#Residuals vs one covariate: use data Hospital.csv

data_hos<-read.csv(here::here("R_Code/R - Module 17/Hospital.csv"))

fit3<-lm(LOS~NURSE,data=data_hos)
plot(data_hos$NURSE, fit3$residuals)
abline(h=0, lwd=2, col=2)

#Residuals vs one covariate
fit4=lm(log(LOS)~NURSE,data=data_hos)
plot(data_hos$NURSE, fit4$residuals)
abline(h=0, lwd=2, col=2)


# Quantile-Quantile plot (QQ-plot)

par(mfrow = c(1, 2))

qqnorm(resid(fit1), xlab = "Expected Value", ylab = "Residual", main = "")
qqline(resid(fit1))
title("(a) QQ Plot for Y (Survival)")

qqnorm(resid(fit2), xlab = "Expected Value", ylab = "Residual", main = "")
qqline(resid(fit2))
title("(d) QQ Plot lnY (LnSurvival)")


# Obtain all (4) diagnostic plots: EASIER and fast check of the MLR diagnostics
# Plot the regression object
par(mfrow=c(2,2))
plot(fit2)


#################################################################################
#                       Box-Cox transformation                                  #
#################################################################################


library(MASS)
fit1 <- lm(Survival ~ Bloodclot, data=data_surg)
boxcox(fit1)  # default grid of lambdas is -2 to 2 by 0.1

# Could change grid of lambda values
boxcox(fit1, lambda = seq(-3, 3, by=0.25) ) 


# Box Cox for multiple regression
mult.fit1 <- lm(Survival ~ Bloodclot + Progindex + Enzyme + Liver + Age + Gender + Alcmod + Alcheav, data=data_surg) 
summary(mult.fit1)

boxcox(mult.fit1) 
plot(mult.fit1)


mult.fit2 <- lm(Lnsurvival ~ Bloodclot + Progindex + Enzyme + Liver + Age + Gender + Alcmod + Alcheav, data=data_surg) 
summary(mult.fit2)
boxcox(mult.fit2) 




#####################################################################################
#           Outliers, leverage and influential points                               #
#####################################################################################

# rstandard function gives the INTERNALLY studentized residuals 

stu_res<-rstandard(mult.fit1)
outliers_y<-stu_res[abs(stu_res)>2.5]

# Measures of influence:
# Gives DFFITS, Cook's Distance, Hat diagonal elements, and others.

influence.measures(mult.fit1)

# Look at the Cook's distance lines and notice obs 5 and 28 as potential Y outliers/influential points

par(mfrow=c(2,2))
plot(mult.fit1)

# Examine results with and without observations 5 and 28 that have very high survivals (>2000)
summary(mult.fit1)
plot(data_surg$Bloodclot, data_surg$Survival)
surg_only5_28<-data_surg[c(5,28),]


# Remove observations 5 and 28
surg_no5_28<-data_surg[c(-5,-28),]
plot(surg_no5_28$Bloodclot, surg_no5_28$Survival)
mult.fit_no5_28<- lm(Survival ~ Bloodclot + Progindex + Enzyme + Liver + Age + Gender + Alcmod + Alcheav, data=surg_no5_28) 

summary(mult.fit_no5_28)

influence.measures(mult.fit_no5_28)

par(mfrow=c(2,2))
plot(mult.fit_no5_28)


#####################################################################################
#                             Multicollinearity                                     #
#####################################################################################
 
# Generate data
set.seed(1)

data.multi = data.frame(age = round(rnorm(100, 30, 10),2)) %>%
  mutate(height = 25 + 2*age + rnorm(100, 0, 15),
         age_copy=age, age_new=age + rnorm(100, 0, 2))

# Correlation matrix for all variables
round(cor(data.multi),3)

# Scatter plot matrix for all variables
pairs(data.multi)


# Fit a simple linear regression with original age
reg1<-lm(height~age, data=data.multi)
summary(reg1)

# Fit a linear regression with original age and age_copy
reg2<-lm(height~age+age_copy, data=data.multi)
summary(reg2)

# Fit a linear regression with original age and age_new
reg3<-lm(height~age+age_new, data=data.multi)
summary(reg3)

# Fit a linear regression with original age, age_copy and age_new
reg4<-lm(height~age+age_copy+age_new, data=data.multi)
summary(reg4)


###
# Calculate the variance inflation factor (VIF)
vif(reg4)


# Data Surgical
# Checking for multicollinearity: VIF
vif(mult.fit_no5_28)

```
## Selection
```{r selection}

################################################################
#                   Biostatistical Methods I                   #
#               Model Building/Variable Selection              #
################################################################

library(dplyr)
library(HH)
library(leaps)
library(corrplot)                      # Correlation plot

# Read data Surgical.csv

data_surg<-read.csv(here::here("R_Code/R - Module 20/Surgical.csv"))
names(data_surg)

# Attach function lets you access the variables directly
attach(data_surg)


# Explorations: scatter plot matrix and pairwise correlations
pairs(data_surg)

# Helps identify collinearity 
cor(data_surg)


# Another way to generate the correlation matrix
cor_mat <- round(cor(data_surg),2)

# Vizualize correlation matrix
# Requires library(corrplot) and library(dplyr) 


par(mar=c(4,5,1,1))                                            # Enlarge the dimension of the plot
cor(data_surg[,c(1,2,3,4,5,10)])%>%                            # Select only continuous variables to calculate correlation 
  corrplot(method = "circle", type = "upper", diag=FALSE)


# Boxplots for each variable
# par(mar=c(3,3,1,1)) # if margins are too large
par(mfrow=c(2,3))
boxplot(Lnsurvival, main='Lnsurvival')
boxplot(Age, main='Age')
boxplot(Bloodclot,main='Bloodclot' )
boxplot(Progindex, main='Progindex')
boxplot(Enzyme, main='Enzyme')
boxplot(Liver, main='Liver')

# Make a data frame with ln(Survival) as the only outcome.
surg<-data.frame(data_surg[,-9])

# Fit a regression using all predictors
mult.fit <- lm(Lnsurvival ~ Bloodclot + Progindex + Enzyme + Liver + Age + Gender + Alcmod + Alcheav, data=surg)
summary (mult.fit)

# Same thing
mult.fit <- lm(Lnsurvival ~ ., data=surg)
summary(mult.fit)

#############################################################################
# Backward elimination: take out non-significant variables 'one at a time'  #
# starting with the highest p-value                                         #
#############################################################################

# No Liver
step1<-update(mult.fit, . ~ . -Liver)
summary(step1)

# No Alcmod
step2<-update(step1, . ~ . -Alcmod)
summary(step2)

# No Age
step3<-update(step2, . ~ . -Age)
summary(step3)

# No Gender
step4<-update(step3, . ~ . -Gender)
summary(step4)

# What are the effects on R2 and adj-R2? Any effect on the coefficients?


#########################################################################################
# Forward elimination: Reversed backward elimination starting with the lowest p-value   #                                                            
#########################################################################################

library(faraway)
library(broom)


############################################################################
#  Stepwise regreession                                                    #   
# 'Step' function uses AIC criterion for var selection and the default     #
#  option is 'backward'.                                                   #
#  Step is a simplified version of stepAIC()                               #
############################################################################

mult.fit <- lm(Lnsurvival ~ ., data=surg)
step(mult.fit, direction='backward')


############################################################################
#                         Test-based procedures                           #
############################################################################

# Leaps function provides all-subsets analysis

# Printing the 2 best models of each size, using the Cp criterion:
leaps(x = surg[,1:8], y = surg[,9], nbest=2, method="Cp")


# Printing the 2 best models of each size, using the adjusted R^2 criterion:
leaps(x = surg[,1:8], y = surg[,9], nbest=2, method="adjr2")

# Summary of models for each size (one model per size)
# Function regsubsets() performs a subset slection by identifying the "best" model that contains
# a certain number of predictors. By default "best" is chosen using SSE/RSS (smaller is better).


b<-regsubsets(Lnsurvival ~ ., data=surg)
   (rs<-summary(b))

# This function also returns R2, Cp, BIC for each "best" model.
# Let's take a look at these values.

# Plots of Cp and Adj-R2 as functions of parameters

par(mar=c(4,4,1,1))
par(mfrow=c(1,2))

plot(2:9, rs$cp, xlab="No of parameters", ylab="Cp Statistic")
abline(0,1)

plot(2:9, rs$adjr2, xlab="No of parameters", ylab="Adj R2")


# AIC of the 6-predictor model:

multi.fit6 <- lm(Lnsurvival ~ Age + Gender + Bloodclot + Alcheav + Progindex + Enzyme, data=data_surg)
AIC(multi.fit6)

# BIC
AIC(multi.fit6, k = log(length(Lnsurvival)))

# AIC of the 4-predictor model:
multi.fit4 <- lm(Lnsurvival ~ Enzyme + Progindex + Alcheav + Bloodclot)
AIC(multi.fit4)

# How do the 6- and 4-predictors models compare in terms of AIC, R-adj, Cp?

#############################################################################
#   A more compact way to look at the test-based results                    #
#############################################################################

best <- function(model, ...) 
{
  subsets <- regsubsets(formula(model), model.frame(model), ...)
  subsets <- with(summary(subsets),
                  cbind(p = as.numeric(rownames(which)), which, rss, rsq, adjr2, cp, bic))
  
  return(subsets)
}  


# Select the 'best' model of all subsets for 4-predictor model
round(best(multi.fit4, nbest = 1), 4)


# Checking the model assumptions for the model: Lnsurvival ~ Enzyme + Progindex + Alcheav + Bloodclot
par(mfrow=c(2,2))
plot(multi.fit4)



```

## Validation 
[Validation on P8105](https://p8105.com/cross_validation.html)

[Bootstrap on P8105](https://p8105.com/bootstrapping.html)

```{r validataion}

################################################################
#                   Biostatistical Methods I                   #
#                      Model Validation                        #
################################################################


# Read data Surgical.csv and keep only lnSurvival as outcome
data_surg<-read.csv(here::here("R_Code/R - Module 20/Surgical.csv"))
surg_lnSurv<-data.frame(data_surg[,-9])

library(caret)

# Use 5-fold validation and create the training sets

set.seed(1)
data_train<-trainControl(method="cv", number=5)

# Fit the 4-variables model that we discussed in previous lectures
model_caret<-train(Lnsurvival ~ Enzyme + Progindex + Alcheav + Bloodclot,
                   data=surg_lnSurv,
                   trControl=data_train,
                   method='lm',
                   na.action=na.pass)
  
# Model predictions using 4 parts of the data for training 
model_caret

# RMSE=2206257, R-squared = 0.7996792 - good % variation accounted for
# Both MAE and RMSE express average model prediction error in units of the variable of interest.
# Both metrics can range from 0 to ??? and are indifferent to the direction of errors (lower is better).
# MAE is steady and RMSE increases as the variance associated with the frequency distribution of error magnitudes also increases (large errors are not desired).


# Model coefficients
model_caret$finalModel

# Examine model prediction for each fold
model_caret$resample

# Look at standard deviation around the Rsquared value by examining the R-squared from each fold.
sd(model_caret$resample$Rsquared)



# Let's look at the model using all data, no CV
full_model<-lm(Lnsurvival ~ Enzyme + Progindex + Alcheav + Bloodclot, data=surg_lnSurv)
summary(full_model)

# Full data had an Rsquared=0.8299; what was the CV result for R-squared?
# What about MSE values? Values are comparable.


# Exercise for you: try 'caret' and split the data 50-50


#####################################################
#                        LOOCV                      #
#####################################################

# Use glm() instead of lm() because of cv.glm() function
glm.fit<-glm(Lnsurvival ~ Enzyme + Progindex + Alcheav + Bloodclot, data=surg_lnSurv)


library(boot)                          # For cv.glm()
cv.err<-cv.glm(surg_lnSurv,glm.fit)

# The two delta values should be similar: we use the first one-the raw cross-validation estimate of prediction error.
# The second value is bias corrected
cv.err$delta   

# Next steps are to fit models with other predictor combinations and compare the CVs

########################################################################
# Bootstrap to assess the variability of model estimates: b0 and b1    #
########################################################################

boot.fn<-function(data, index){
	return(coef(lm(Lnsurvival ~ Enzyme + Progindex + Alcheav + Bloodclot, data=surg_lnSurv, subset=index)))
}

# Our usual regression, no bootstrap yet
boot.fn(surg_lnSurv,1:54)


# Compute the estimates by sampling with replacement
# Sample chooses 54 observations from 54, with replacement
# Might have duplicates
set.seed(1)

# One draw
boot.fn(surg_lnSurv,sample(54,54,replace=T))

# Use function boot() to repeat the sampling 10000 times.
# Repeat 10000 times to get the estimates, SEs ad bias

boot(surg_lnSurv, boot.fn, 10000)

# How does it compare to the original (non-bootstrap) estimates?


########################################################################
#                   Model validation: criteria                         #
########################################################################

library(MPV)                           # For PRESS criterion

newsummary <- function(model)
{
    list('coefs'    = round(t(summary(model)$coef[, 1:2]), 4),
         'criteria' = cbind('SSE'   = anova(model)["Residuals", "Sum Sq"],
                            'PRESS' = PRESS(model),
                            'MSE'   = anova(model)["Residuals", "Mean Sq"],
                            'Rsq'   = summary(model)$adj.r.squared))
}

newsummary(lm(Lnsurvival ~ Bloodclot + Alcheav + Progindex + Enzyme, surg_lnSurv))



####################################################################
#               Data Generation and Cross Validation              #
####################################################################


## Generate 100 training sets each of size 50 from a polynomial regression model
# Fit a sequence of cubic spline models with degrees of freedom from 1 to 20.

set.seed(3)

gen_data <- function(n, beta, sigma_eps) {
  eps <- rnorm(n, 0, sigma_eps)
  x <- sort(runif(n, 0, 100))
  X <- cbind(1, poly(x, degree = (length(beta) - 1), raw = TRUE))
  y <- as.numeric(X %*% beta + eps)
  
  return(data.frame(x = x, y = y))
}

# Fit the models
require(splines)

n_rep <- 100
df <- 1:20
beta <- c(5, 0.3, 0.001, -3e-05)           
n_test <- 100
n_train<-50
sigma_eps <- 0.5

xy <- res <- list()

# Test data
xy_test <- gen_data(n_test, beta, sigma_eps)

# Train data
for (i in 1:n_rep) {
  xy[[i]] <- gen_data(n_train, beta, sigma_eps)
  x <- xy[[i]][, "x"]
  y <- xy[[i]][, "y"]
  res[[i]] <- apply(t(df), 2, function(degf) lm(y ~ ns(x, df = degf)))
}

# Plot true and training data with overlaid models

#dev.off()

x <- xy[[1]]$x
X <- cbind(1, poly(x, degree = (length(beta) - 1), raw = TRUE))
y <- xy[[1]]$y
plot(y ~ x, col = "gray", lwd = 2)
lines(x, X %*% beta, lwd = 3, col = 1)                        #True model
lines(x, fitted(res[[1]][[1]]), lwd = 3, col = 2)
lines(x, fitted(res[[1]][[3]]), lwd = 3, col = 3)
lines(x, fitted(res[[1]][[20]]), lwd = 3, col = 4)
legend(x = "bottomright", legend = c("True function", "Linear fit (df = 1)", "Good model (df = 3)", 
                                     "Overfitted model (df = 20)"), lwd = rep(3, 4), col = c(1,2,3,4), cex = 0.85)




# k-fold cross-validation using simulated data. Generate 100 observations and choose k=10.
# The plot shows the training, test errors, CV and standard errors of the individual prediction error for each of the k=10 parts.

n_train <- 100
xy <- gen_data(n_train, beta, sigma_eps)
x <- xy$x
y <- xy$y

fitted_models <- apply(t(df), 2, function(degf) lm(y ~ ns(x, df = degf)))
mse <- sapply(fitted_models, function(obj) deviance(obj)/nobs(obj))

n_test <- 100
xy_test <- gen_data(n_test, beta, sigma_eps)
pred <- mapply(function(obj, degf) predict(obj, data.frame(x = xy_test$x)), 
               fitted_models, df)
te <- sapply(as.list(data.frame(pred)), function(y_hat) mean((xy_test$y - y_hat)^2))

n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n_train))
cv_tmp <- matrix(NA, nrow = n_folds, ncol = length(df))
for (k in 1:n_folds) {
  test_i <- which(folds_i == k)
  train_xy <- xy[-test_i, ]
  test_xy <- xy[test_i, ]
  x <- train_xy$x
  y <- train_xy$y
  fitted_models <- apply(t(df), 2, function(degf) lm(y ~ ns(x, df = degf)))
  x <- test_xy$x
  y <- test_xy$y
  pred <- mapply(function(obj, degf) predict(obj, data.frame(ns(x, df = degf))), 
                 fitted_models, df)
  cv_tmp[k, ] <- sapply(as.list(data.frame(pred)), function(y_hat) mean((y - 
                                                                           y_hat)^2))
}
cv <- colMeans(cv_tmp)

require(Hmisc)

plot(df, mse, type = "l", lwd = 2, col = gray(0.4), ylab = "Prediction error", 
     xlab = "Degrees of freedom (log scaled)", main = paste0(n_folds, 
                                                                     "-fold Cross-Validation"), log = "x")

# For a better vizualization of the errors, you can change the ylim=c(0.01,1)

lines(df, te, lwd = 2, col = "darkred", lty = 2)
cv_sd <- apply(cv_tmp, 2, sd)/sqrt(n_folds)
errbar(df, cv, cv + cv_sd, cv - cv_sd, add = TRUE, col = "steelblue2", pch = 19, 
       lwd = 0.5)
lines(df, cv, lwd = 2, col = "steelblue2")
points(df, cv, col = "steelblue2", pch = 19)
legend(x = "topright", legend = c("Training error", "Test error", "Cross-validation error"), 
       lty = c(1, 2, 1), lwd = rep(2, 3), col = c(gray(0.4), "darkred", "steelblue2"), cex = 0.85)

```

